{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\nimport os\nimport gc\nimport json\nimport random\nimport itertools\nimport collections\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tqdm.auto import tqdm\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras import backend as K\nfrom collections import defaultdict,Counter\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow_addons.optimizers import RectifiedAdam\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold,RepeatedKFold,RepeatedStratifiedKFold\nfrom transformers import BertTokenizer, TFBertModel, BertConfig, BertModel, TFDistilBertModel, DistilBertConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,5)\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU : {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS : {REPLICAS}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image(file):\n    file = f'bpps/{file}.npy' if 'npy' not in file else file\n    data = np.load(file)\n    _ = plt.title(file.split('/')[-1],color='white')\n    _ = plt.imshow(data)\n    _ = plt.axis('off')\n    _ = plt.colorbar()\n    return data\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(54)\n\ndef get_bppm(file):\n    return np.load(f'bpps/{file}.npy')\n\ndef get_count(x,v):\n    return x.count(v)\n\ndef padding(x,col):\n    PAD_LEN = 130\n    to_len = 130 - len(x) \n    if col == 'sequence':\n        return x + ('N' * to_len)\n    elif col == 'structure':\n        return x + ('#' * to_len)\n    elif col == 'predicted_loop_type':\n        return x + ('Z' * to_len)\n\ndef get_dataset(data,label,params):\n    if label is not None:\n        data = tf.data.Dataset.from_tensor_slices((data,label)).batch(params['batch_size'])\n        if params['shuffle']:\n            data = data.shuffle(1024)\n        if params['repeat']:\n            data = data.repeat()\n        return data.prefetch(AUTO)\n    else:\n        data = tf.data.Dataset.from_tensor_slices((data)).batch(params['batch_size']).prefetch(AUTO)\n        return data\n    \ndef explode(df, lst_cols, fill_value=''):\n    if lst_cols and not isinstance(lst_cols, list):\n        lst_cols = [lst_cols]\n    idx_cols = df.columns.difference(lst_cols)\n    lens = df[lst_cols[0]].str.len()\n\n    if (lens > 0).all():\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .loc[:, df.columns]\n    else:\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n          .loc[:, df.columns]\n    \n    \ndef stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices\n        \n        \n    \ndef get_error(preds):\n    val = pd.read_json('train.json', lines=True)\n\n    val_data = []\n    for mol_id in val['id'].unique():\n        sample_data = val.loc[val['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(68):\n            sample_dict = {\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n                           }\n            \n            val_data.append(sample_dict)\n            \n    val_data = pd.DataFrame(val_data)\n    val_data = val_data.merge(preds, on='id_seqpos')\n\n    rmses = []\n    mses = []\n    print('column\\t\\tRMSE\\t\\tMSE')\n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n        rmses.append(rmse)\n        mses.append(mse)\n        print(f'{col}\\t{rmse:0.5f}\\t\\t{mse:0.5f}')\n        \n    print(f'Mean RMSE : {np.mean(rmses):0.5f}')\n    print(f'Mean MSE  : {np.mean(mses):0.5f}')\n    print('\\n')\n    \ndef format_predictions(test_df, test_preds, val=False):\n    preds = []\n    \n    for df, preds_ in zip(test_df, test_preds):\n        for i, uid in enumerate(df['id']):\n            single_pred = preds_[i]\n\n            single_df = pd.DataFrame(single_pred, columns=pred_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            if val:\n                single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n\n            preds.append(single_df)\n    return pd.concat(preds).groupby('id_seqpos').mean().reset_index() if TTA else pd.concat(preds)\n\n\ndef aug_data(df):\n    aug_df = pd.read_csv('../augmented-data-for-stanford-covid-vaccine/48k_augment.csv')\n    aug_df['aug'] = 1\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n    \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n    \n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df['aug'] = 0\n    df = df.append(new_df[df.columns])\n    return df\n\nSEED = 42\ndef extend_sequence(df,p):\n    np.random.seed(SEED)\n    n = int(df.shape[0]*p)\n    sample = df.sample(n).copy(deep=True)\n    sample['seq_length'] = 130\n    sample['seq_scored'] = 91\n    sample[sequence_cols] = (sample[sequence_cols].applymap(lambda x:x[:65]) +\n                             sample[sequence_cols].applymap(lambda x:x[-65:]))\n    \n    sample[pred_cols] = (sample[pred_cols].applymap(lambda x:x[:45]) +\n                             sample[pred_cols].applymap(lambda x:x[-46:]))\n    \n    df = df.append(sample,ignore_index=True)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = './../input/stanford-covid-vaccine/'\nos.chdir(PATH)\nos.listdir()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DENOISE = False\nRUN_TEST = False\nFLIP_AUG = False\nTTA = False\nLONG_SEQ = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_json('train.json',lines=True).drop(columns=['index']).sort_values(by='id')\ntest = pd.read_json('test.json',lines=True).drop(columns=['index']).sort_values(by='id')\n\nsubmission = pd.read_csv('sample_submission.csv')\nnpy = ['bpps/'+x for x in os.listdir('bpps')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_cols = ['sequence','structure','predicted_loop_type']\npred_cols = ['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']\n\n\nif TTA:\n    train = aug_data(train)\n    test = aug_data(test)\n\n    \nif LONG_SEQ:\n    train = extend_sequence(train,0.25 if TTA else 1)\n    \n\ntrain['flip'] = 0\nif FLIP_AUG:\n    train_flip = pd.read_json('train.json',lines=True).drop(columns=['index']).sort_values(by='id')\n    train_flip = train_flip[train_flip['SN_filter'] >= 1]\n    train_flip['flip'] = 1\n    train_flip[sequence_cols + pred_cols] = train_flip[sequence_cols + pred_cols].applymap(lambda x:x[::-1])\n    train = train.append(train_flip,ignore_index=True)\n    del train_flip\n\nif DENOISE:\n    train = train[train['SN_filter']>=1]\n    \nif RUN_TEST:\n    train = train.sample(300)\n    test = test.sample(300)\n    \nprint(f'train shape : {train.shape}')\nprint(f'test shape : {test.shape}')\n\nprint(f'Null values if train data : {train.isnull().sum().sum()}')\nprint(f'Null values if test data : {test.isnull().sum().sum()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\"\"\"def get_ohe(x,i):\n    vec_len = {'(':3, ')':3, '.':3,\n           'A':4, 'U':4, 'G':4, 'C':4,\n           'B':7, 'E':7, 'H':7, 'I':7, 'M':7, 'S':7, 'X':7}\n    x = np.zeros(vec_len[x],dtype=np.int32)\n    x[i] = 1\n    return x\nmapping = dict()\nmapping.update({x:get_ohe(x,i) for i,x in enumerate(['A','U','G','C'])})\nmapping.update({x:get_ohe(x,i) for i,x in enumerate(['(',')','.'])})\nmapping.update({x:get_ohe(x,i) for i,x in enumerate(list('BEHIMSX'))})\nfor c in ['sequence','structure','predicted_loop_type']:\n    train[c] = train[c].apply(list)\n    test[c] = test[c].apply(list)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train.columns.tolist()\ndrop_cols = [x for x in cols if 'error' in x]\ntrain.drop(columns=drop_cols,inplace=True)\n\nseq_values = ['A','U','G','C']\nstruct_values = ['.','(',')']\nloop_values = ['B', 'E', 'H', 'I', 'S', 'X','M']\n\nseq_map = dict(zip(seq_values,range(len(seq_values))))\nstruct_map = dict(zip(struct_values,range(len(struct_values))))\nloop_map = dict(zip(loop_values,range(len(loop_values))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_comb = seq_values + []\nfor p in itertools.permutations(seq_values,2):\n    seq_comb.append(''.join(p))\n    \nstruct_comb = struct_values + []\nfor p in itertools.permutations(struct_values,2):\n    struct_comb.append(''.join(p))\n    \nloop_comb = loop_values + []\nfor p in itertools.permutations(loop_values,2):\n    loop_comb.append(''.join(p))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in ['sequence','structure','predicted_loop_type']:\n    train[c] = train[c].apply(list)\n    test[c] = test[c].apply(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in seq_comb:\n    train[f'{c}_content'] = train['sequence'].apply(lambda x:x.count(c))/107\n    test[f'{c}_content'] = test['sequence'].apply(lambda x:x.count(c))/130\n    \nfor c in struct_comb:\n    train[f'{c}_content'] = train['structure'].apply(lambda x:x.count(c))/107\n    test[f'{c}_content'] = test['structure'].apply(lambda x:x.count(c))/130\n    \nfor c in loop_comb:\n    train[f'{c}_content'] = train['predicted_loop_type'].apply(lambda x:x.count(c))/107\n    test[f'{c}_content'] = test['predicted_loop_type'].apply(lambda x:x.count(c))/130","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bpm_feature(df,len_):\n    bpm_max = []\n    bpm_sum = []\n    bpm_upb = []\n    bpm_mean = []\n    bpm_std = []\n    bpm_nb = []\n    \n    bpps_nb_mean = 0.077522\n    bpps_nb_std = 0.08914\n    \n    for idx in tqdm(df.id.values):\n        bpm_ar = get_bppm(idx)\n        bpm_max.append(np.max(bpm_ar,axis=1)/len_)\n        bpm_sum.append(np.sum(bpm_ar,axis=1)/len_)\n        bpm_upb.append(1-np.sum(bpm_ar,axis=1)/len_)\n        bpm_mean.append(np.mean(bpm_ar,axis=1)/len_)\n        bpm_std.append(np.std(bpm_ar,axis=1)/len_)\n        bpps_nb_ = (bpm_ar > 0).sum(axis=0) / bpm_ar.shape[0]\n        bpps_nb_ = (bpps_nb_ - bpps_nb_mean) / bpps_nb_std\n        bpm_nb.append(bpps_nb_)\n        \n    return bpm_max,bpm_sum,bpm_upb,bpm_mean,bpm_std,bpm_nb\n\nbpm_max_tr,bpm_sum_tr,bpm_upb_tr,bpm_mean_tr,bpm_std_tr,bpm_nb_tr = get_bpm_feature(train,107)\nbpm_max_tst,bpm_sum_tst,bpm_upb_tst,bpm_mean_tst,bpm_std_tst,bpm_nb_tst = get_bpm_feature(test,130)\n\ntrain['bpm_max'] = bpm_max_tr\ntrain['bpm_sum'] = bpm_sum_tr\ntrain['bpm_upb'] = bpm_upb_tr\ntrain['bpm_std'] = bpm_std_tr\ntrain['bpm_nb'] = bpm_nb_tr\ntrain['bpm_mean'] = bpm_mean_tr\n\ntest['bpm_max'] = bpm_max_tst\ntest['bpm_sum'] = bpm_sum_tst\ntest['bpm_upb'] = bpm_upb_tst\ntest['bpm_std'] = bpm_std_tst\ntest['bpm_nb'] = bpm_nb_tst\ntest['bpm_mean'] = bpm_mean_tst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_short = train[train['seq_scored'] == 68]\ntrain_long = train[train['seq_scored'] == 91]\n\n\npublic_test = test[test['seq_scored'] == 68]\nprivate_test = test[test['seq_scored'] == 91]\n\nprint(f'train short shape : {train_short.shape}')\nprint(f'train long shape : {train_long.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\ndesc_tr = train.describe().T.sort_values(by='std')\nstd0_tr = desc_tr[desc_tr['std'] == 0].index.tolist() \n\ndesc_tst = test.describe().T.sort_values(by='std')\nstd0_tst = desc_tst[desc_tst['std'] == 0].index.tolist() \n\nstd0_cols = set(std0_tr + std0_tst)\n#std0_cols.remove('seq_length')\n#std0_cols.remove('seq_scored')\nstd0_cols.remove('flip')\ntrain.drop(columns=std0_cols,inplace=True)\n\n#train_short.drop(columns=std0_cols,inplace=True)\n#train_long.drop(columns=std0_cols,inplace=True)\n\npublic_test.drop(columns=std0_cols,inplace=True)\nprivate_test.drop(columns=std0_cols,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_short = train_short.sort_values(by='sequence').reset_index(drop=True)\n#train_long = train_long.sort_values(by='sequence').reset_index(drop=True)\n\n#display(train_short.head(1))\n#display(train_long.head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SPLITS = 5\ntr_data = train[train['signal_to_noise'] >= 0.25]\nif not TTA:\n    print('RepeatedStratifiedKFold')\n    folds = RepeatedStratifiedKFold(n_splits=SPLITS,n_repeats=1,random_state=42)\n    for i,(_,val_idx) in enumerate(folds.split(tr_data['id'],tr_data['SN_filter'])):\n        train.loc[val_idx,'folds'] = i\n    #for i,(_,val_idx) in enumerate(folds.split(tr_short['id'],tr_short['SN_filter'])):\n    #    train_short.loc[val_idx,'folds'] = i\n    #for i,(_,val_idx) in enumerate(folds.split(tr_long['id'],tr_long['SN_filter'])):\n    #    train_long.loc[val_idx,'folds'] = i\n            \nelse:\n    print('stratified_group_k_fold')\n    for i,(_,val_idx) in enumerate(stratified_group_k_fold(train,train['SN_filter'],train['id'],SPLITS)):\n        train.loc[val_idx,'folds'] = i\n    #for i,(_,val_idx) in enumerate(stratified_group_k_fold(tr_short,tr_short['SN_filter'],tr_short['id'],SPLITS)):\n    #    train_short.loc[val_idx,'folds'] = i\n    #for i,(_,val_idx) in enumerate(stratified_group_k_fold(tr_long,tr_long['SN_filter'],tr_long['id'],SPLITS)):\n    #    train_long.loc[val_idx,'folds'] = i\n    \n#del tr_short,tr_long\n    \n#print(train_short.folds.value_counts())\n#print(train_long.folds.value_counts())\nprint(train.folds.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\nEMB_SIZE = 256\nDROPOUT = 0.1\nBATCH_SIZE = 64*REPLICAS\nLR = 0.01*REPLICAS\nHEADS = 4\n\ndef MCRMSE(y_true, y_pred):\n    columnwise_mse = tf.reduce_mean(tf.square(y_true-y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=1)\n\ndef RMSE(y_true,y_pred):\n    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true,y_pred)\n    return tf.math.sqrt(loss)\n\n\ntoken2int = {x:i for i, x in enumerate('().AUGCBEHIMSX')}\ndef preprocess_data(df, cols):\n    base_fea1 = np.array(df[cols[:2]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    base_fea2 = np.array(df[cols[2:]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    \n    #bpps_sum_fea = np.array(df['bpm_sum'].to_list())[:,:,np.newaxis]\n    #bpps_max_fea = np.array(df['bpm_max'].to_list())[:,:,np.newaxis]    \n    data = np.concatenate([base_fea1, base_fea2], 2)\n    return data\n\ndef gru_model():\n    return L.Bidirectional(\n        L.GRU(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal')\n    )\n\ndef lstm_model():\n    return L.Bidirectional(\n        L.LSTM(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal')\n    )\n\ndef conv_block(ksize):\n    return L.Conv1D(filters=EMB_SIZE,kernel_size=ksize,padding='same',\n                    activation='swish')\n\ndef attn_block(layer):\n    query = conv_block(1)(layer)\n    value = conv_block(1)(layer)\n    key = conv_block(1)(layer)\n    attn = L.Attention(dropout=DROPOUT,use_scale=True)([query,value,key])\n    \n    return attn\n\n\ndef multi_head(layer):\n    last_axis = layer.shape[-1]\n    assert last_axis % HEADS == 0\n    split = last_axis//HEADS\n\n    heads = [attn_block(layer[:,:,i*split:(i+1)*split]) for i in range(HEADS)]\n    concat = tf.concat(heads,axis=2)\n    \n    return concat\n    \n\ndef process_block(seq,seq_len):\n    seq_emb = L.Embedding(len(token2int),EMB_SIZE)(seq)\n    seq_reshape = L.Reshape((seq_len,seq_emb.shape[2]*seq_emb.shape[3]))(seq_emb)\n    seq_drop = L.SpatialDropout1D(DROPOUT)(seq_reshape)\n    seq_conv = conv_block(2)(seq_drop)\n    \n    lstm1 = lstm_model()(seq_conv)\n    attn1 = multi_head(lstm1)\n    \n    lstm2 = lstm_model()(attn1)\n    attn2 = multi_head(lstm2)\n    \n    lstm3 = lstm_model()(attn2)\n    attn3 = multi_head(lstm3)\n    \n    lstm4 = lstm_model()(attn3)\n    attn4 = multi_head(lstm4)\n    \n    return attn4\n\ndef build_model(seq_len, pred_len):\n    inp = L.Input(shape=(seq_len,3)) \n    \n    #seq1 = tf.expand_dims(inp[:,:,0],axis=-1)\n    #seq2 = tf.expand_dims(inp[:,:,1],axis=-1)\n    #seq3 = tf.expand_dims(inp[:,:,2],axis=-1)\n    \n    seq1 = inp[:,:,:2]\n    seq2 = inp[:,:,2:]\n    #seq3 = inp[:,:,3:]\n    \n    #seq3_conv = L.Conv1D(filters=EMB_SIZE,kernel_size=1,padding='same')(seq3)\n    #seq3_norm = L.BatchNormalization()(seq3_conv)\n\n    seq1_out = process_block(seq1,seq_len)\n    seq2_out = process_block(seq2,seq_len)\n    #seq3_out = process_block(seq3,seq_len)\n    \n    concat = L.Concatenate(axis=2)([seq1_out, seq2_out])\n    concat = L.BatchNormalization()(concat)\n\n    out = concat[:,:pred_len]\n    out = L.Dense(5,activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=[inp],outputs=out)\n    model.compile(optimizer=RectifiedAdam(lr=LR),loss=MCRMSE)\n    return model\n    \nmodel = build_model(68,68)\ntf.keras.utils.plot_model(model,to_file='./../../working/model.png',show_shapes=True,dpi=55)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = sequence_cols\nprivate_preds = np.zeros((private_test.shape[0],130,5))\npublic_preds = np.zeros((public_test.shape[0],107,5))\nval_preds = np.zeros((train.shape[0],107,5))\n\nprivate_data =  preprocess_data(private_test,cols)\npublic_data =  preprocess_data(public_test,cols)\n\nnon_fil_score = []\nfil_score = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nEPOCHS = 30\npred_cols_1 = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\nfor i in range(SPLITS):\n    print(f'Training on fold {i}')\n    K.clear_session()\n    \n    tr_df = train[train['signal_to_noise'] > 0.25].query(f'folds != {i}')\n    valid_df1 = train[train['signal_to_noise'] > 0.25].query(f'folds == {i}')\n    valid_df2 = train[train['signal_to_noise'] >= 1].query(f'folds == {i}')\n    \n    \n    tr_values = preprocess_data(tr_df,['sequence','structure','predicted_loop_type'])\n    tr_label = np.array(tr_df[pred_cols].values.tolist()).transpose(0,2,1)\n    \n    \n    valid_val = preprocess_data(valid_df1,['sequence','structure','predicted_loop_type'])\n    valid_label = np.array(valid_df1[pred_cols].values.tolist()).transpose(0,2,1)\n    valid_data1 = tf.data.Dataset.from_tensor_slices((valid_val,valid_label)).batch(BATCH_SIZE).prefetch(AUTO)\n    \n    valid_val = preprocess_data(valid_df2,['sequence','structure','predicted_loop_type'])\n    valid_label = np.array(valid_df2[pred_cols].values.tolist()).transpose(0,2,1)\n    valid_data2 = tf.data.Dataset.from_tensor_slices((valid_val,valid_label)).batch(BATCH_SIZE).prefetch(AUTO)\n    \n    weight = np.log1p(tr_df['signal_to_noise'] + 0.01)/2\n    #weight = np.log1p((tr_df['signal_to_noise'].max()-tr_df['signal_to_noise']) + 0.01)/2.0\n    \n    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                       factor=0.85,\n                                                       patience=2,\n                                                       verbose=True)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'./../../working/fold_{i}_best_weight.h5',\n                                                    monitor='val_loss',\n                                                    save_weights_only=True,\n                                                    save_best_only=True,\n                                                    mode='min',\n                                                    verbose=True)\n\n    print(f'Building models on distribution strategy mode')\n    with strategy.scope():\n        train_model = build_model(107,68)\n        model_short = build_model(107,107)\n        model_long = build_model(130,130)\n\n    train_model.fit(tr_values,tr_label,\n                    epochs=EPOCHS,\n                    sample_weight = weight,\n                    batch_size=BATCH_SIZE,\n                    validation_data = valid_data1, #[valid_val,valid_label],\n                    callbacks=[lr_schedule,checkpoint],\n                    verbose=True)\n    \n    print(f'Loading fold {i} best weights')\n    train_model.load_weights(f'./../../working/fold_{i}_best_weight.h5')\n    model_short.load_weights(f'./../../working/fold_{i}_best_weight.h5')\n    model_long.load_weights(f'./../../working/fold_{i}_best_weight.h5')\n    \n    print(f'Predicting validation data')\n    print(f'----------Non-Filtered Data-----------')\n    non_fil_score.append(train_model.evaluate(valid_data1,verbose=True))\n    print(f'----------Filtered Data-----------')\n    fil_score.append(train_model.evaluate(valid_data2,verbose=True))\n    \n    print(f'Predicting public test data')\n    public_preds += model_short.predict(public_data,verbose=True)/SPLITS\n    \n    print(f'Predicting private test data')\n    private_preds += model_long.predict(private_data,verbose=True)/SPLITS\n    \n    print('#'*100)\n    print('#'*100)\n    print('#'*100)\n    \n    del tr_df,valid_df1,valid_df2,tr_values,tr_label,valid_val,\n    del valid_label,valid_data1,valid_data2,weight\n    del lr_schedule,checkpoint,train_model,model_short,model_long\n    \n    gc.collect()\n    \nprint(f'Mean MCRMSE filtered data : {np.mean(fil_score):0.5f}')\nprint(f'Mean MCRMSE Non filtered data : {np.mean(non_fil_score):0.5f}')\nprint(f'Mean MCRMSE total data : {(np.mean(non_fil_score)+np.mean(fil_score))/2.0:0.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(non_fil_score,label='non_fil_score')\nplt.plot(fil_score,label='fil_score')\nplt.xticks([0,1,2,3,4],color='white')\nplt.yticks(color='white')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_predictions(test_df, test_preds, val=False):\n    preds = []\n    \n    for df, preds_ in zip(test_df, test_preds):\n        for i, uid in enumerate(df['id']):\n            single_pred = preds_[i]\n\n            single_df = pd.DataFrame(single_pred, columns=pred_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            if val:\n                single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n\n            preds.append(single_df)\n    return pd.concat(preds).groupby('id_seqpos').mean().reset_index() if TTA else pd.concat(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = [public_preds,private_preds]\ntest_df = [public_test,private_test]\nsub_preds = format_predictions(test_df, preds_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_preds = sub_preds[['id_seqpos','reactivity',\n                       'deg_Mg_pH10','deg_pH10',\n                       'deg_Mg_50C','deg_50C']]\nif TTA:\n    sub_preds = submission[['id_seqpos']].merge(sub_preds,on=['id_seqpos'],how='left')\nprint(sub_preds.shape)\nsub_preds.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_file = './../../working/submission_autoencoder_seq_struct_long_short_20.csv'\nsub_preds.to_csv(sub_file,index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Garbage","metadata":{}},{"cell_type":"code","source":"def get_train_fold(df_short,df_long,f):\n    tr_short = df_short[df_short['signal_to_noise'] >= 0.25].query(f'folds != {f}')\n    tr_long = df_long[df_long['signal_to_noise'] >= 0.25 ].query(f'folds != {f}')\n    \n    tr_short_data = preprocess_data(tr_short,sequence_cols)\n    tr_short_label = np.array(tr_short[pred_cols].values.tolist()).transpose(0,2,1)\n    tr_short_weight = np.log1p(tr_short['signal_to_noise'] + 0.01)/2.0\n    \n    tr_long_data = preprocess_data(tr_long,sequence_cols)\n    tr_long_label = np.array(tr_long[pred_cols].values.tolist()).transpose(0,2,1)\n    tr_long_weight = np.log1p(tr_long['signal_to_noise'] + 0.01)/2.0\n    \n    return  tr_short_data,tr_short_label,tr_short_weight,tr_long_data,tr_long_label,tr_long_weight\n\n\ndef get_valid_fold(df_short,df_long,f):\n    val_short = df_short[df_short['signal_to_noise'] >= 0.25].query(f'folds == {f}')\n    val_long = df_long[df_long['signal_to_noise'] >= 0.25].query(f'folds == {f}')\n    \n    val_short_data = preprocess_data(val_short,sequence_cols)\n    val_short_label = np.array(val_short[pred_cols].values.tolist()).transpose(0,2,1)\n    val_short = tf.data.Dataset.from_tensor_slices((val_short_data,val_short_label)).batch(BATCH_SIZE).prefetch(AUTO)\n    \n    val_long_data = preprocess_data(val_long,sequence_cols)\n    val_long_label = np.array(val_long[pred_cols].values.tolist()).transpose(0,2,1)\n    val_long = tf.data.Dataset.from_tensor_slices((val_long_data, val_long_label)).batch(BATCH_SIZE).prefetch(AUTO)\n    \n    return val_short, val_long","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"gc.collect()\nEPOCHS = 30\nscore_long = []\nscore_short = []\n\nfor i in range(SPLITS):\n    print(f'Training on fold {i}')\n    K.clear_session()\n    tr_short,tr_short_label,tr_short_weight,tr_long,tr_long_label,tr_long_weight = get_train_fold(train_short,train_long,i)\n    val_short, val_long = get_valid_fold(train_short,train_long,i)\n    \n    \n    lr_schedule_short = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                       factor=0.85,\n                                                       patience=3,\n                                                       verbose=True)\n    #lr_schedule_long = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n    #                                                   factor=0.85,\n    #                                                   patience=3,\n    #                                                   verbose=True)\n    \n    #checkpoint_long = tf.keras.callbacks.ModelCheckpoint(filepath=f'./../../working/fold_{i}_best_weight_long.h5',\n    #                                                monitor='val_loss',\n    #                                                save_weights_only=True,\n    #                                                save_best_only=True,\n    #                                                mode='min',\n    #                                                verbose=True)\n    checkpoint_short = tf.keras.callbacks.ModelCheckpoint(filepath=f'./../../working/fold_{i}_best_weight_short.h5',\n                                                    monitor='val_loss',\n                                                    save_weights_only=True,\n                                                    save_best_only=True,\n                                                    mode='min',\n                                                    verbose=True)\n\n    print(f'Building models on distribution strategy mode')\n    with strategy.scope():\n        train_model_short = build_model(107,68)\n        #train_model_long = build_model(130,91)\n        \n        short_seq_model_short = build_model(107,107) #Short sequences with short model weights\n        #short_seq_model_long = build_model(107,107)  #Short sequences with long model weights\n        \n        long_seq_model_short = build_model(130,130)  #Long sequences with short model weights\n        #long_seq_model_long = build_model(130,130)   #Long sequences with long model weights\n    \n    print('-'*50)\n    #print('Training LONG model...')\n    #train_model_long.fit(tr_long,tr_long_label,\n    #                epochs=EPOCHS,\n    #                sample_weight = tr_long_weight,\n    #                batch_size=BATCH_SIZE,\n    #                validation_data = val_long,\n    #                callbacks=[lr_schedule_long,checkpoint_long],\n    #                verbose=True)\n    \n    print('Training SHORT model...')\n    train_model_short.fit(tr_short,tr_short_label,\n                    epochs=EPOCHS,\n                    sample_weight = tr_short_weight,\n                    batch_size=BATCH_SIZE,\n                    validation_data = val_short,\n                    callbacks=[lr_schedule_short,checkpoint_short],\n                    verbose=True)\n    \n    print('-'*50)\n    \n    print(f'Loading fold {i} SHORT model best weight')\n    train_model_short.load_weights(f'./../../working/fold_{i}_best_weight_short.h5')\n    short_seq_model_short.load_weights(f'./../../working/fold_{i}_best_weight_short.h5')\n    long_seq_model_short.load_weights(f'./../../working/fold_{i}_best_weight_short.h5')\n    \n    \n    #print(f'Loading fold {i} LONG model best weight')\n    #train_model_short.load_weights(f'./../../working/fold_{i}_best_weight_long.h5')\n    #short_seq_model_long.load_weights(f'./../../working/fold_{i}_best_weight_long.h5')\n    #long_seq_model_long.load_weights(f'./../../working/fold_{i}_best_weight_long.h5')\n    \n    print('-'*50)\n    \n    print(f'Predicting validation data')\n    #print(f'++++++++++  Long Seq Data   +++++++++++')\n    #score_long.append(train_model_long.evaluate(val_long,verbose=True))\n    \n    print(f'++++++++++  Short Seq Data  +++++++++++')\n    score_short.append(train_model_short.evaluate(val_short,verbose=True))\n    \n    print('-'*50)\n    print(f'Predicting public test data')\n    public_preds += short_seq_model_short.predict(public_data,verbose=True)\n    #public_preds += short_seq_model_long.predict(public_data,verbose=True)\n    public_preds /= SPLITS\n    \n    print(f'Predicting private test data')\n    private_preds += long_seq_model_short.predict(private_data,verbose=True)\n    #private_preds += long_seq_model_long.predict(private_data,verbose=True)\n    private_preds /= SPLITS\n    \n    print('#'*100)\n    print('#'*100)\n    print('#'*100)\n    \n    del checkpoint_short,lr_schedule_short,val_short, #checkpoint_long,lr_schedule_long\n    del tr_short,tr_short_label,tr_short_weight#,val_long,tr_long,tr_long_label,tr_long_weight\n    del train_model_short#,train_model_long \n    del short_seq_model_short#,short_seq_model_long\n    del long_seq_model_short#,long_seq_model_long\n    \n    gc.collect()\n    \nprint(f'Mean MCRMSE SHORT data : {np.mean(score_short):0.5f}')\n#print(f'Mean MCRMSE LONG data : {np.mean(score_long):0.5f}')\n#print(f'Mean MCRMSE Total data : {((np.mean(score_long)+np.mean(score_short))/2.0):0.5f}')\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"K.clear_session()\nEMB_SIZE = 256\nDROPOUT = 0.1\nBATCH_SIZE = 64*REPLICAS\nLR = 0.01*REPLICAS\n\ndef MCRMSE(y_true, y_pred):\n    columnwise_mse = tf.reduce_mean(tf.square(y_true-y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=1)\n\ndef RMSE(y_true,y_pred):\n    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true,y_pred)\n    return tf.math.sqrt(loss)\n\n\ntoken2int = {x:i for i, x in enumerate('().AUGCBEHIMSX')}\ndef preprocess_data(df, cols):\n    base_fea1 = np.array(df[cols[:2]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    base_fea2 = np.array(df[cols[2:]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    #base_fea3 = np.array(df[cols[:3:2]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    \n    bpps_sum_fea = np.array(df['bpm_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpm_max'].to_list())[:,:,np.newaxis]\n    #bpps_std_fea = np.array(df['bpm_std'].to_list())[:,:,np.newaxis]\n    #bpps_upb_fea = np.array(df['bpm_upb'].to_list())[:,:,np.newaxis]\n    data = np.concatenate([base_fea1, base_fea2, bpps_sum_fea,bpps_max_fea], 2)\n    return data\n\ndef gru_model():\n    return L.Bidirectional(\n        L.GRU(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal')\n    )\n\ndef lstm_model():\n    return L.Bidirectional(\n        L.LSTM(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal')\n    )\n\ndef conv_block(ksize):\n    return L.Conv1D(filters=EMB_SIZE,kernel_size=ksize,padding='same',\n                    activation='swish')\n\ndef attn_block(layer):\n    query = conv_block(1)(layer)\n    key = conv_block(1)(layer)\n    value = conv_block(1)(layer)\n    \n    attn = L.Attention(dropout=DROPOUT,use_scale=True)([query,value,key])\n    \n    add = L.Add()([layer,attn])\n    norm = L.LayerNormalization()(add)\n    drop = L.SpatialDropout1D(DROPOUT)(norm)\n    \n    return drop\n\ndef process_block(seq,seq_len):\n    seq_emb = L.Embedding(len(token2int),EMB_SIZE)(seq)\n    seq_reshape = L.Reshape((seq_len,seq_emb.shape[2]*seq_emb.shape[3]))(seq_emb)\n    seq_drop = L.SpatialDropout1D(DROPOUT)(seq_reshape)\n    #seq_conv = conv_block(2)(seq_drop)\n    \n    lstm1 = lstm_model()(seq_drop)\n    attn1 = attn_block(lstm1)\n    \n    lstm2 = lstm_model()(attn1)\n    attn2 = attn_block(lstm2)\n    \n    lstm3 = lstm_model()(attn2)\n    attn3 = attn_block(lstm3)\n    \n    #lstm3_mul = L.Multiply()([lstm1,lstm3])\n    #lstm3_norm = L.BatchNormalization()(lstm3_mul)\n    #lstm3_drop = L.SpatialDropout1D(DROPOUT)(lstm3_norm)\n    \n    lstm4 = lstm_model()(attn3)\n    attn4 = attn_block(lstm4)\n    \n    #lstm4_mul = L.Multiply()([lstm2,lstm4])\n    #lstm4_norm = L.BatchNormalization()(lstm4_mul)\n    #lstm4_drop = L.SpatialDropout1D(DROPOUT)(lstm4_norm)\n    \n    return attn4\n\ndef build_model(seq_len, pred_len):\n    inp = L.Input(shape=(seq_len,6))\n    seq1 = inp[:,:,:2]\n    seq2 = inp[:,:,1:3]\n    \n    seq3 = inp[:,:,3:]\n    seq3_conv = conv_block(2)(seq3)\n    \n    seq1_out = process_block(seq1,seq_len)\n    seq2_out = process_block(seq2,seq_len)\n    \n    concat = L.Concatenate(axis=2)([seq1_out, seq2_out, seq3_conv])\n    concat = L.BatchNormalization()(concat)\n    out = L.SpatialDropout1D(DROPOUT)(concat)\n\n    out = out[:,:pred_len]\n    out = L.Dense(5,activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=inp,outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LR),loss=MCRMSE)\n    return model\n    \nmodel = build_model(68,68)\ntf.keras.utils.plot_model(model,to_file='./../../working/model.png',show_shapes=True,dpi=55)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"K.clear_session()\nEMB_SIZE = 256\nDROPOUT = 0.1\nBATCH_SIZE = 64*REPLICAS\nLR = 0.01*REPLICAS\n\ndef MCRMSE(y_true, y_pred):\n    columnwise_mse = tf.reduce_mean(tf.square(y_true-y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(columnwise_mse), axis=1)\n\ndef RMSE(y_true,y_pred):\n    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true,y_pred)\n    return tf.math.sqrt(loss)\n\n\ntoken2int = {x:i for i, x in enumerate('().AUGCBEHIMSX')}\ndef preprocess_data(df, cols):\n    base_fea1 = np.array(df[cols[:2]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    base_fea2 = np.array(df[cols[2:]].applymap(lambda x:[token2int[i] for i in x]).values.tolist()).transpose(0,2,1)\n    \n    bpps_sum_fea = np.array(df['bpm_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpm_max'].to_list())[:,:,np.newaxis]\n    #bpps_mean_fea = np.array(df['bpm_mean'].to_list())[:,:,np.newaxis]\n    bpps_upb_fea = np.array(df['bpm_upb'].to_list())[:,:,np.newaxis]\n    data = np.concatenate([base_fea1, base_fea2, bpps_sum_fea, bpps_max_fea, bpps_upb_fea], 2)\n    return data\n\ndef gru_model():\n    return L.Bidirectional(L.GRU(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal'))\n\ndef lstm_model():\n    return L.Bidirectional(L.LSTM(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal'))\n\n\ndef process_block(seq,seq_len):\n    seq_emb = L.Embedding(len(token2int),EMB_SIZE)(seq)\n    seq_reshape = L.Reshape((seq_len,seq_emb.shape[2]*seq_emb.shape[3]))(seq_emb)\n    seq_drop = L.SpatialDropout1D(DROPOUT)(seq_reshape)\n    seq_conv = L.Conv1D(filters=EMB_SIZE,kernel_size=2,padding='same')(seq_drop)\n    \n    lstm1 = lstm_model()(seq_conv)\n    lstm2 = lstm_model()(lstm1)\n    \n    conv_block = L.Conv1D(filters=EMB_SIZE,kernel_size=1,padding='same',\n                         activation=tf.keras.activations.swish)\n    \n    query = conv_block(lstm2)\n    key = conv_block(lstm2)\n    value = conv_block(lstm2)\n    attn = L.Attention(dropout=DROPOUT,use_scale=True)([query,value,key])\n    \n    lstm3 = lstm_model()(attn)\n    lstm_mul = L.Multiply()([lstm1,lstm3])\n    lstm_norm = L.BatchNormalization()(lstm_mul)\n    lstm4 = lstm_model()(lstm_norm)\n    \n    return lstm4\n\ndef build_model(seq_len, pred_len):\n    inp = L.Input(shape=(seq_len,6))\n    seq1 = inp[:,:,:2]\n    seq2 = inp[:,:,1:3]\n    \n    seq3 = inp[:,:,3:]\n    seq3_dense = L.Conv1D(filters=EMB_SIZE,kernel_size=2,padding='same',\n                          activation=tf.keras.activations.swish)(seq3)\n    \n    seq1_out = process_block(seq1,seq_len)\n    seq2_out = process_block(seq2,seq_len)\n    \n    concat = L.Concatenate(axis=2)([seq1_out, seq2_out, seq3_dense])\n    out = L.SpatialDropout1D(DROPOUT)(concat)\n    out = L.Dense(256)(out)\n    out = out[:,:pred_len]\n    out = L.Dense(5,activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=inp,outputs=out)\n    model.compile(optimizer=RectifiedAdam(lr=LR),loss=MCRMSE)\n    return model\n    \nmodel = build_model(68,68)\ntf.keras.utils.plot_model(model,to_file='./../../working/model.png',show_shapes=True,dpi=55)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"comb = np.array(train[sequence_cols].values.tolist()).transpose(0,2,1)\ncombined_seq = []\nfor i in range(len(comb)):\n    join = []\n    for j in range(len(comb[i])):\n        join.append(''.join(comb[i][j]))\n    combined_seq.append(q)\ntrain['combined_seq'] = combined_seq\n\n\n\ncomb = np.array(test[sequence_cols].values.tolist()).transpose(0,2,1)\ncombined_seq = []\nfor i in range(len(comb)):\n    join = []\n    for j in range(len(comb[i])):\n        join.append(''.join(comb[i][j]))\n    combined_seq.append(q)\ntest['combined_seq'] = combined_seq\"\"\"","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"%%capture\ntrain['sequence'] = train['sequence'].apply(lambda x:[seq_map[i] for i in x])\npublic_test['sequence'] = public_test['sequence'].apply(lambda x:[seq_map[i] for i in x])\nprivate_test['sequence'] = private_test['sequence'].apply(lambda x:[seq_map[i] for i in x])\n\n\ntrain['structure'] = train['structure'].apply(lambda x:[struct_map[i] for i in x])\npublic_test['structure'] = public_test['structure'].apply(lambda x:[struct_map[i] for i in x])\nprivate_test['structure'] = private_test['structure'].apply(lambda x:[struct_map[i] for i in x])\n\n\ntrain['predicted_loop_type'] = train['predicted_loop_type'].apply(lambda x:[loop_map[i] for i in x])\npublic_test['predicted_loop_type'] = public_test['predicted_loop_type'].apply(lambda x:[loop_map[i] for i in x])\nprivate_test['predicted_loop_type'] = private_test['predicted_loop_type'].apply(lambda x:[loop_map[i] for i in x])\n\n\ntrain['combined_seq'] = train['combined_seq'].apply(lambda x:[pairs[i] for i in x])\npublic_test['combined_seq'] = public_test['combined_seq'].apply(lambda x:[pairs[i] for i in x])\nprivate_test['combined_seq'] = private_test['combined_seq'].apply(lambda x:[pairs[i] for i in x])\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nBEST MODEL 1\nK.clear_session()\nEMB_SIZE = 256\nDROPOUT = 0.1\nBATCH_SIZE = 64*REPLICAS\nLR = 0.01*REPLICAS\n\ndef gru_model():\n    return L.Bidirectional(L.GRU(EMB_SIZE//2,dropout=DROPOUT,return_sequences=True,kernel_initializer='orthogonal'))\n\ndef lstm_model():\n    return L.Bidirectional(L.LSTM(EMB_SIZE//2,return_sequences=True,kernel_initializer='orthogonal'))\n\n\ndef build_model(seq_len, pred_len):\n    inp = L.Input(shape=(seq_len,4))\n    seq1 = inp[:,:,:2]\n    seq2 = inp[:,:,2:]\n    seq2_dense = L.Conv1D(filters=EMB_SIZE,kernel_size=2,padding='same')(seq2)\n    \n    seq1_emb = L.Embedding(len(token2int),EMB_SIZE)(seq1)\n    seq1_reshape = L.Reshape((seq_len,seq1_emb.shape[2]*seq1_emb.shape[3]))(seq1_emb)\n    seq1_conv = L.Conv1D(filters=EMB_SIZE,kernel_size=2,padding='same')(seq1_reshape)\n    \n    lstm1 = lstm_model()(seq1_conv)\n    lstm2 = lstm_model()(lstm1)\n    \n    conv_block = L.Conv1D(filters=EMB_SIZE,kernel_size=1,padding='same')\n    query = conv_block(lstm2)\n    key = conv_block(lstm2)\n    value = conv_block(lstm2)\n    attn = L.Attention(dropout=DROPOUT)([query,value,key])\n    \n    concat = L.Concatenate(axis=2)([attn,seq2_dense])\n    out = concat[:,:pred_len]\n    out = L.Dense(5,activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=inp,outputs=out)\n    model.compile(optimizer=RectifiedAdam(lr=LR),loss=MCRMSE)\n    return model\n    \nmodel = build_model(68,68)\ntf.keras.utils.plot_model(model,to_file='./../../working/model.png',show_shapes=True,dpi=55)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}